{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c79cc8-a6f5-4ac5-957f-cb5184611277",
   "metadata": {},
   "source": [
    "## sales_data from the Walmart supplier list\n",
    "## customer_data and store_data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27e24738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/sridharmalladi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d48687-fb26-490a-ad0c-8da36841992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all three datasets\n",
    "import pandas as pd\n",
    "customer_data = pd.read_csv(\"customer_data.csv\") \n",
    "store_data = pd.read_csv(\"store_data.csv\")\n",
    "sales_data = pd.read_csv(\"sales_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd66f22c-6081-4957-adf8-5e21ee35def1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in customer_data:\n",
      "customer_id      0\n",
      "age            215\n",
      "gender         205\n",
      "date             0\n",
      "location       204\n",
      "num_items      197\n",
      "avg_price      223\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking for the missing values in customer data\n",
    "missing_values = customer_data.isnull().sum()\n",
    "print(\"Missing values in customer_data:\")\n",
    "print(missing_values)\n",
    "#Significant number of missing values to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27830e",
   "metadata": {},
   "source": [
    "## Handling the Missing Values in the customer_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e4ca1-a77a-4429-85ab-2173b4b9e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original dataset missing values:\n",
      "customer_id      0\n",
      "age            215\n",
      "gender         205\n",
      "date             0\n",
      "location       204\n",
      "num_items      197\n",
      "avg_price      223\n",
      "dtype: int64\n",
      "\n",
      "Cleaned dataset missing values:\n",
      "customer_id    0\n",
      "age            0\n",
      "gender         0\n",
      "date           0\n",
      "location       0\n",
      "num_items      0\n",
      "avg_price      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Cleaning the dataset from variable to variable\n",
    "import numpy as np\n",
    "#Importing SimpleImputer for handling missing values\n",
    "from sklearn.impute import SimpleImputer  \n",
    "\n",
    "def clean_customer_data(df):\n",
    "    #Creating a copy of the original dataframe to preserve the original data\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    #Handling Missing Ages\n",
    "    #Creating an imputer object that will replace missing values with the median\n",
    "    #Using median instead of mean because it's more robust to outliers which can be present\n",
    "    age_imputer = SimpleImputer(strategy='median')\n",
    "    #fit_transform() function learns the median from the data and applies the imputation\n",
    "    df_cleaned['age'] = age_imputer.fit_transform(df_cleaned[['age']])\n",
    "    \n",
    "    #Handling Missing Gender\n",
    "    #For categorical data like gender, we use the mode\n",
    "    df_cleaned['gender'] = df_cleaned['gender'].fillna(df_cleaned['gender'].mode()[0])\n",
    "    \n",
    "    #Handling Missing Location\n",
    "    #Similar to gender, we use the most common location to fill missing values\n",
    "    df_cleaned['location'] = df_cleaned['location'].fillna(df_cleaned['location'].mode()[0])\n",
    "    \n",
    "    #Handling Missing Number of Items\n",
    "    #Using median because it's less sensitive to extreme values\n",
    "    num_items_imputer = SimpleImputer(strategy='median')\n",
    "    df_cleaned['num_items'] = num_items_imputer.fit_transform(df_cleaned[['num_items']])\n",
    "    \n",
    "    #Handling Missing Average Prices\n",
    "    #Median is especially good for prices as they often have outliers\n",
    "    price_imputer = SimpleImputer(strategy='median')\n",
    "    df_cleaned['avg_price'] = price_imputer.fit_transform(df_cleaned[['avg_price']])\n",
    "    \n",
    "    # Return the cleaned dataset\n",
    "    return df_cleaned\n",
    "\n",
    "#Applying the cleaning function to your dataset\n",
    "customer_data_cleaned = clean_customer_data(customer_data)\n",
    "\n",
    "#Printing before and after summaries to verify the cleaning\n",
    "print(\"\\nOriginal dataset missing values:\")\n",
    "print(customer_data.isnull().sum())  \n",
    "print(\"\\nCleaned dataset missing values:\")\n",
    "print(customer_data_cleaned.isnull().sum())  \n",
    "#Saving the cleaned dataset\n",
    "customer_data_cleaned.to_csv('customer_data_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eb8be5b-4d8c-48bd-bad3-6021901db0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in store_data:\n",
      "store_id                 0\n",
      "location                 0\n",
      "state                    0\n",
      "category                 0\n",
      "size_sqft                0\n",
      "mom_growth               1\n",
      "yoy_growth               0\n",
      "negative_equity          0\n",
      "delinquency              0\n",
      "inventory_turnover       0\n",
      "employee_count           0\n",
      "customer_satisfaction    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking for the missing values in customer data\n",
    "missing_values = store_data.isnull().sum()\n",
    "print(\"Missing values in store_data:\")\n",
    "print(missing_values)\n",
    "#Since the value is negligible we can move forward with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a98691c-a536-4186-aa6c-e99a580bfdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in sales_data:\n",
      "transactionID     0\n",
      "storeID           0\n",
      "date              0\n",
      "state             0\n",
      "season            0\n",
      "category          0\n",
      "is_holiday        0\n",
      "total_amount      0\n",
      "profit_margin     0\n",
      "items_count       0\n",
      "payment_method    0\n",
      "customer_type     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking for the missing values in customer data\n",
    "missing_values = sales_data.isnull().sum()\n",
    "print(\"Missing values in sales_data:\")\n",
    "print(missing_values)\n",
    "#no missing values found!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc380ef",
   "metadata": {},
   "source": [
    "## Adding the Latitudes and longitudes to the store_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "184a830c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading store data...\n",
      "\n",
      "Getting coordinates...\n",
      "Processed 5 stores...\n",
      "Processed 10 stores...\n",
      "Processed 15 stores...\n",
      "Processed 20 stores...\n",
      "Processed 25 stores...\n",
      "Processed 30 stores...\n",
      "Error with address: 615 S Cumberland St, Lebanon, AL 37087\n",
      "Error message: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=615+S+Cumberland+St%2C+Lebanon%2C+AL+37087%2C+USA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Processed 35 stores...\n",
      "Processed 40 stores...\n",
      "Processed 45 stores...\n",
      "Processed 50 stores...\n",
      "Processed 55 stores...\n",
      "Processed 60 stores...\n",
      "\n",
      "Results:\n",
      "Total stores: 60\n",
      "Coordinates found: 45\n",
      "\n",
      "Sample of updated data:\n",
      "                                       location   latitude   longitude\n",
      "0     3826 S Suncoast Blvd, Homosassa, FL 34448        NaN         NaN\n",
      "1         1936 N Lecanto Hwy, Lecanto, FL 34461  28.892694  -82.483937\n",
      "2     6885 S Suncoast Blvd, Homosassa, FL 34446        NaN         NaN\n",
      "3  2461 E Gulf To Lake Hwy, Inverness, FL 34453        NaN         NaN\n",
      "4           2100 Vista Way, Oceanside, CA 92054  33.181334 -117.347285\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "#Reading store data\n",
    "print(\"Loading store data...\")\n",
    "stores = pd.read_csv('store_data.csv')\n",
    "\n",
    "#Initializing geocoder for location optimization \n",
    "geolocator = Nominatim(user_agent=\"retail_store_locator\")\n",
    "\n",
    "#Function to get coordinates with error handling perfectly\n",
    "def get_coordinates(address):\n",
    "    try:\n",
    "        #Adding USA as that is where our data is from\n",
    "        full_address = f\"{address}, USA\"\n",
    "        location = geolocator.geocode(full_address)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error with address: {address}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None, None\n",
    "    finally:\n",
    "        #Managing the API limits\n",
    "        time.sleep(1)\n",
    "\n",
    "#Getting coordinates for each store\n",
    "print(\"\\nGetting coordinates...\")\n",
    "coordinates = []\n",
    "for idx, address in enumerate(stores['location']):\n",
    "    lat, lon = get_coordinates(address)\n",
    "    coordinates.append((lat, lon))\n",
    "    if (idx + 1) % 5 == 0:  # Progress update every 5 stores\n",
    "        print(f\"Processed {idx + 1} stores...\")\n",
    "\n",
    "#Adding new columns\n",
    "stores['latitude'] = [coord[0] for coord in coordinates]\n",
    "stores['longitude'] = [coord[1] for coord in coordinates]\n",
    "\n",
    "#Saving updated data\n",
    "stores.to_csv('store_data.csv', index=False)\n",
    "\n",
    "#Results\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Total stores: {len(stores)}\")\n",
    "print(f\"Coordinates found: {stores['latitude'].notna().sum()}\")\n",
    "print(\"\\nSample of updated data:\")\n",
    "print(stores[['location', 'latitude', 'longitude']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5148dfe3",
   "metadata": {},
   "source": [
    "## Looking for missing latitudes and longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13aa09f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stores missing coordinates:\n",
      "3826 S Suncoast Blvd, Homosassa, FL 34448 (FL)\n",
      "6885 S Suncoast Blvd, Homosassa, FL 34446 (FL)\n",
      "2461 E Gulf To Lake Hwy, Inverness, FL 34453 (FL)\n",
      "2800 NC-24 #87, Cameron, NC 28326 (NC)\n",
      "1550 Nashville Rd, Franklin, KY 42134 (KY)\n",
      "2100 88th St, North Bergen, NY 07047 (NY)\n",
      "141 Washington Ave Ext, Albany, NY 12205 (NY)\n",
      "300 Pleasant Grove Rd Ste 600, Mount Juliet, TN 37122 (TN)\n",
      "304 S Rockwood Dr, Cabot, AR 72023 (AR)\n",
      "615 S Cumberland St, Lebanon, AL 37087 (AL)\n",
      "300 Pleasant Grove Rd Ste 600, Mount Juliet, IL 61522 (IL)\n",
      "4424 Lebanon Pike, Hermitage, IL 37076 (IL)\n",
      "5511 Murfreesboro Rd, La Vergne, IL 37086 (IL)\n",
      "2000 Old Fort Pkwy, Murfreesboro, IL 37129 (IL)\n",
      "200 Walmart Way, Avon, IN 46123 (IN)\n"
     ]
    }
   ],
   "source": [
    "#Looking for missing latitudes and longitudes\n",
    "import pandas as pd\n",
    "\n",
    "# Read store data\n",
    "stores = pd.read_csv('store_data.csv')\n",
    "\n",
    "# Find missing locations\n",
    "missing_coords = stores[stores['latitude'].isna() | stores['longitude'].isna()]\n",
    "\n",
    "# Print missing locations\n",
    "print(\"Stores missing coordinates:\")\n",
    "for idx, store in missing_coords.iterrows():\n",
    "    print(f\"{store['location']} ({store['state']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb504f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All coordinates have been added successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read store data\n",
    "stores = pd.read_csv('store_data.csv')\n",
    "\n",
    "#Coordinates for missing locations\n",
    "missing_coordinates = {\n",
    "    \"3826 S Suncoast Blvd, Homosassa, FL 34448\": (28.7892, -82.5958),\n",
    "    \"6885 S Suncoast Blvd, Homosassa, FL 34446\": (28.7666, -82.5752),\n",
    "    \"2461 E Gulf To Lake Hwy, Inverness, FL 34453\": (28.8372, -82.3738),\n",
    "    \"2800 NC-24 #87, Cameron, NC 28326\": (35.3276, -79.2534),\n",
    "    \"1550 Nashville Rd, Franklin, KY 42134\": (36.7081, -86.5777),\n",
    "    \"2100 88th St, North Bergen, NY 07047\": (40.8144, -74.0023),\n",
    "    \"141 Washington Ave Ext, Albany, NY 12205\": (42.6901, -73.8490),\n",
    "    \"300 Pleasant Grove Rd Ste 600, Mount Juliet, TN 37122\": (36.1991, -86.5186),\n",
    "    \"304 S Rockwood Dr, Cabot, AR 72023\": (34.9744, -92.0167),\n",
    "    \"615 S Cumberland St, Lebanon, AL 37087\": (33.5186, -86.8104),\n",
    "    \"300 Pleasant Grove Rd Ste 600, Mount Juliet, IL 61522\": (41.8781, -87.6298),\n",
    "    \"4424 Lebanon Pike, Hermitage, IL 37076\": (41.8781, -87.6298),\n",
    "    \"5511 Murfreesboro Rd, La Vergne, IL 37086\": (41.8781, -87.6298),\n",
    "    \"2000 Old Fort Pkwy, Murfreesboro, IL 37129\": (41.8781, -87.6298),\n",
    "    \"200 Walmart Way, Avon, IN 46123\": (39.7684, -86.1581)\n",
    "}\n",
    "\n",
    "#Updating only missing coordinates\n",
    "for location, (lat, lon) in missing_coordinates.items():\n",
    "    mask = stores['location'] == location\n",
    "    stores.loc[mask, 'latitude'] = lat\n",
    "    stores.loc[mask, 'longitude'] = lon\n",
    "\n",
    "#Saving updated data\n",
    "stores.to_csv('store_data.csv', index=False)\n",
    "\n",
    "#Verifying updates\n",
    "still_missing = stores[stores['latitude'].isna() | stores['longitude'].isna()]\n",
    "if len(still_missing) == 0:\n",
    "    print(\"All coordinates have been added successfully!\")\n",
    "else:\n",
    "    print(f\"Still missing coordinates for {len(still_missing)} stores:\")\n",
    "    print(still_missing[['location', 'state']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ae12e-ec62-4f5c-8272-3f5de56c1388",
   "metadata": {},
   "source": [
    "## Adding the temperature and weather conditions using VisualCrossing.com API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73cda16-e9a1-4624-97a9-8f8bb9e9dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def get_weather_data(sales_df, api_key):\n",
    "    base_url = \"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline\"\n",
    "    \n",
    "    state_coords = {\n",
    "        'TX': {'lat': 30.2672, 'lon': -97.7431},  #Austin\n",
    "        'FL': {'lat': 28.5383, 'lon': -81.3792},  #Orlando\n",
    "        'CA': {'lat': 34.0522, 'lon': -118.2437}, #LA\n",
    "        'NC': {'lat': 35.7796, 'lon': -78.6382},  #Raleigh\n",
    "        'GA': {'lat': 33.7490, 'lon': -84.3880},  #Atlanta\n",
    "        'KY': {'lat': 38.2527, 'lon': -85.7585},  #Louisville\n",
    "        'NY': {'lat': 40.7128, 'lon': -74.0060},  #NYC\n",
    "        'TN': {'lat': 36.1627, 'lon': -86.7816},  #Nashville\n",
    "        'AR': {'lat': 34.7465, 'lon': -92.2896},  #Little Rock\n",
    "        'AL': {'lat': 33.5186, 'lon': -86.8104},  #Birmingham\n",
    "        'UT': {'lat': 40.7608, 'lon': -111.8910}, #Salt Lake City\n",
    "        'IL': {'lat': 41.8781, 'lon': -87.6298},  #Chicago\n",
    "        'IN': {'lat': 39.7684, 'lon': -86.1581},  #Indianapolis\n",
    "        'MO': {'lat': 38.6270, 'lon': -90.1994},  #St. Louis\n",
    "        'NM': {'lat': 35.6870, 'lon': -105.9378}  #Santa Fe\n",
    "    }\n",
    "\n",
    "    #Adding week number and weather columns\n",
    "    sales_df['week'] = pd.to_datetime(sales_df['date']).dt.isocalendar().week\n",
    "    sales_df['temperature'] = None\n",
    "    sales_df['weather_condition'] = None\n",
    "    \n",
    "    for state in sales_df['state'].unique():\n",
    "        print(f\"\\nGetting weather for {state}\")\n",
    "        \n",
    "        for week in sorted(sales_df[sales_df['state'] == state]['week'].unique()):\n",
    "            try:\n",
    "                \n",
    "                sample_date = sales_df[\n",
    "                    (sales_df['state'] == state) & \n",
    "                    (sales_df['week'] == week)\n",
    "                ]['date'].min()\n",
    "                \n",
    "                date_str = sample_date.strftime('%Y-%m-%d')\n",
    "                coords = state_coords[state]\n",
    "                location = f\"{coords['lat']},{coords['lon']}\"\n",
    "                url = f\"{base_url}/{location}/{date_str}?unitGroup=metric&key={api_key}\"\n",
    "                \n",
    "                response = requests.get(url)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    temp = data['days'][0]['temp']\n",
    "                    condition = data['days'][0]['conditions']\n",
    "                    \n",
    "                    mask = (sales_df['state'] == state) & (sales_df['week'] == week)\n",
    "                    sales_df.loc[mask, 'temperature'] = temp\n",
    "                    sales_df.loc[mask, 'weather_condition'] = condition\n",
    "                    \n",
    "                    print(f\"Week {week}: {temp}Â°C, {condition}\")\n",
    "                else:\n",
    "                    print(f\"Error for week {week}: {response.status_code}\")\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed for week {week}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    return sales_df.drop('week', axis=1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Loading data\n",
    "    print(\"Loading data from sales_data.csv...\")\n",
    "    sales_df = pd.read_csv('sales_data.csv')\n",
    "    sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "    \n",
    "    #Adding weather data using the API key\n",
    "    api_key = 'FW7ULVFY572R8U6CJHF5FZEA8'\n",
    "    sales_df = get_weather_data(sales_df, api_key)\n",
    "    \n",
    "    #Saving back to same file\n",
    "    sales_df.to_csv('sales_data.csv', index=False)\n",
    "    print(\"\\nUpdated sales_data_temperature.csv with weather data!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84b417",
   "metadata": {},
   "source": [
    "## Adding the temperature and the weather condition to the sale_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89207f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Loading both files\n",
    "sales_df = pd.read_csv('sales_data.csv')\n",
    "temp_df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "#Merging files wrt date and state\n",
    "sales_df = pd.merge(\n",
    "    sales_df,\n",
    "    temp_df[['date', 'state', 'temperature', 'weather_condition']],\n",
    "    on=['date', 'state']\n",
    ")\n",
    "\n",
    "#Saving updated file\n",
    "sales_df.to_csv('sales_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0651abd",
   "metadata": {},
   "source": [
    "## Calculating the sales per square foot and adding it as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b987ea85-242b-4429-a5b3-336555fec8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sales per Square Foot Summary by Store Category:\n",
      "                           mean     min     max\n",
      "category                                       \n",
      "Discount Store       119.025000   96.10  134.14\n",
      "Neighborhood Market  207.193333  120.65  303.25\n",
      "Supercenter          103.775882   68.04  161.08\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Loading store data\n",
    "store_df = pd.read_csv('store_data.csv')\n",
    "\n",
    "#Calculating sales per square foot\n",
    "store_df['sales_per_sqft'] = (store_df['monthly_sales'] * 12) / store_df['size_sqft']\n",
    "\n",
    "#Rounding to 2 decimal places\n",
    "store_df['sales_per_sqft'] = store_df['sales_per_sqft'].round(2)\n",
    "\n",
    "#Saving to store_data.csv\n",
    "store_df.to_csv('store_data.csv', index=False)\n",
    "\n",
    "#Printing summary\n",
    "print(\"\\nSales per Square Foot Summary by Store Category:\")\n",
    "print(store_df.groupby('category')['sales_per_sqft'].agg(['mean', 'min', 'max']))\n",
    "\n",
    "#The data shows that Neighborhood Markets have the highest sales per \n",
    "#square foot (averaging $207/sqft) because they're smaller stores with focused inventory, \n",
    "#while Supercenters have lower sales per square foot (averaging $104/sqft) despite higher \n",
    "#total sales because they have much larger floor spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885ee1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c958d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
